# -*- coding: utf-8 -*-
"""svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nUShpP2fY01yFy008i9MAUqbXjFfSwqm
"""

import numpy as np
import math
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC

def plot_results(models, titles, X, y, plot_sv=False):
    # Set-up 2x2 grid for plotting.
    fig, sub = plt.subplots(1, len(titles))  # 1, len(list(models)))

    X0, X1 = X[:, 0], X[:, 1]
    xx, yy = make_meshgrid(X0, X1)

    if len(titles) == 1:
        sub = [sub]
    else:
        sub = sub.flatten()
    for clf, title, ax in zip(models, titles, sub):
        # print(title)
        plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
        ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors="k")
        if plot_sv:
            sv = clf.support_vectors_
            ax.scatter(sv[:, 0], sv[:, 1], c='k', s=60)

        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(title)
        ax.set_aspect('equal', 'box')
    fig.tight_layout()
    plt.show()

def make_meshgrid(x, y, h=0.02):
    """Create a mesh of points to plot in

    Parameters
    ----------
    x: data to base x-axis meshgrid on
    y: data to base y-axis meshgrid on
    h: stepsize for meshgrid, optional

    Returns
    -------
    xx, yy : ndarray
    """
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    """Plot the decision boundaries for a classifier.

    Parameters
    ----------
    ax: matplotlib axes object
    clf: a classifier
    xx: meshgrid ndarray
    yy: meshgrid ndarray
    params: dictionary of params to pass to contourf, optional
    """
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

C_hard = 1000000.0  # SVM regularization parameter
C = 10
n = 100

# Data is labeled by a circle


radius = np.hstack([np.random.random(n), np.random.random(n) + 1.5])
angles = 2 * math.pi * np.random.random(2 * n)
X1 = (radius * np.cos(angles)).reshape((2 * n, 1))
X2 = (radius * np.sin(angles)).reshape((2 * n, 1))

X = np.concatenate([X1,X2],axis=1)
Y = np.concatenate([np.ones((n,1)), -np.ones((n,1))], axis=0).reshape([-1])


#Split the data.
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

#Regularization parameter C =10
model_configs = [
    ("Linear SVM", SVC(kernel="linear", C = 10)),
    ("Poly SVM (degree 2)", SVC(kernel="poly", degree=2, C = 10)),
    ("Poly SVM (degree 3)", SVC(kernel="poly", degree=3, C = 10)),
]

# Train models and collect predictions
models = []
titles = []

for title, model in model_configs:
    model.fit(X_train, y_train)
    models.append(model)
    titles.append(title)

# Optionally predict on test set (can store if needed)
# y_preds = [model.predict(X_test) for model in models]

# Plot results
plot_results(models, titles, X_test, y_test, plot_sv=True)

"""Linear SVM (Left Plot)
The linear SVM attempts to find a straight-line decision boundary to separate the two classes. However, the underlying data distribution is not linearly separable — likely one class is concentrated in the center while the other surrounds it. As a result, the linear model misclassifies many points and uses a large number of support vectors. This is a classic case of underfitting, where the model is too simple to capture the structure of the data.

Polynomial SVM (Degree 2, Middle Plot)
The degree-2 polynomial SVM introduces nonlinearity, allowing it to create a circular decision boundary that perfectly fits the concentric pattern in the data. This model captures the true shape of the class distribution, with fewer misclassifications and fewer support vectors. It strikes a good balance between complexity and fit, making it the most appropriate choice for this specific dataset.

Polynomial SVM (Degree 3, Right Plot)
The degree-3 polynomial SVM produces a more complex and jagged decision boundary. While it achieves tight class separation, the intricate shape indicates the model may be overfitting — learning the noise and fine details in the training data rather than the general pattern. The high number of support vectors supports this, as the model depends heavily on many training points to define its boundary. This could lead to poor performance on unseen data.

Same Thing just added a shift (bias) made the Models non-Homo
"""

#Regularization parameter C =10
model_configs = [
    ("Linear SVM", SVC(kernel="linear", C = 10, coef0=1)),
    ("Poly SVM (degree 2)", SVC(kernel="poly", degree=2, C = 10, coef0=1)),
    ("Poly SVM (degree 3)", SVC(kernel="poly", degree=3, C = 10, coef0=1)),
]

# Train models and collect predictions
models = []
titles = []

for title, model in model_configs:
    model.fit(X_train, y_train)
    models.append(model)
    titles.append(title)

# Optionally predict on test set (can store if needed)
# y_preds = [model.predict(X_test) for model in models]

# Plot results
plot_results(models, titles, X_test, y_test, plot_sv=True)

"""In the updated plots where coef0=1 is used for the polynomial SVMs, the decision boundaries become noticeably smoother and more centered compared to the previous versions with coef0=0. Previously, the degree-3 polynomial created overly complex, star-shaped boundaries, indicating overfitting, while the degree-2 model was slightly rigid. By setting coef0=1, the models become non-homogeneous, allowing them to include linear and constant terms in the kernel expansion. This added flexibility enables both models to better capture the underlying circular pattern of the data, producing clean, rounded boundaries that tightly enclose the central class. As a result, the decision surfaces now align more naturally with the class structure, showing how a simple change in coef0 can dramatically improve model behavior and generalization."""

#Changing some of the labels to 1 instead of -1.
y_noisy = y.copy()
flip_mask = (y_noisy == -1) & (np.random.rand(len(y_noisy)) < 0.1)
y_noisy[flip_mask] = 1

#Fitting the 2 models.
model_poly = SVC(kernel = "poly", degree=2, C=10)
model_poly.fit(X, y_noisy)


model_rbf_1 = SVC(kernel = "rbf", gamma=1,C=10)
model_rbf_5 = SVC(kernel = "rbf", gamma=5,C=10)
model_rbf_10 = SVC(kernel = "rbf", gamma=10,C=10)
model_rbf_1.fit(X, y_noisy)
model_rbf_5.fit(X, y_noisy)
model_rbf_10.fit(X, y_noisy)


plot_results([model_rbf_10.fit(X, y_noisy),model_poly.fit(X, y_noisy)], ["rbf w/Gamma=10", "Poly 2nd degree"], X_test, y_test, plot_sv=True)
plot_results([model_rbf_10.fit(X, y_noisy), model_rbf_5.fit(X, y_noisy), model_rbf_1.fit(X, y_noisy)], ["Gamma = 10", "Gamma = 5", "Gamma = 1"], X_test, y_test, plot_sv=True)

"""The polynomial SVM with degree 2 generalizes better than the RBF SVM with γ = 10 on noisy data, as evidenced by its smooth and stable decision boundary that ignores small label perturbations. The RBF kernel with a high γ overfits the noise, producing complex, erratic boundaries that fail to generalize. As γ is reduced, the RBF model becomes more robust — its boundary smooths out and begins to resemble that of the polynomial model. This demonstrates how γ controls the flexibility of the RBF kernel, and that moderate γ values (like 1) can offer a good balance between flexibility and generalization."""